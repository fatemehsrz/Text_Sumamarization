{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Text Summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install bert-extractive-summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summarizer.bert import Summarizer ,TransformerSummarizer\n",
    "import time\n",
    "\n",
    "text= \"\"\"Text classification is a classic problem in Natural Language Processing (NLP). \n",
    "The task is to assign predefined categories to a given text sequence. \n",
    "An important intermediate step is the text representation. \n",
    "Previous work uses various neural models to learn text representation, including convolution models \n",
    "(Kalchbrenner et al., 2014; Zhang et al.,2015), \n",
    "recurrent models (Liu et al., 2016; Yogatama et al.,2017; Seo et al., 2017), \n",
    "and attention mechanisms (Yang et al., 2016; Lin et al., 2017). Alternatively, \n",
    "substantial work has shown that pre-trained models on large corpus are beneficial for text classification and \n",
    "other NLP tasks, which can avoid training a new model from scratch. \n",
    "One kind of pre-trained models is the word embeddings, such as word2vec (Mikolov et al., 2013) \n",
    "and GloVe (Pennington et al., 2014), or the contextualized word embeddings, such as \n",
    "CoVe (McCann et al., 2017) and ELMo (Peters et al., 2018). These word embeddings are often used as \n",
    "additional features for the main task. Another kind of pre-training models is sentence level. \n",
    "Howard and Ruder (2018) propose ULMFiT, a fine-tuning method for pre-trained language model \n",
    "that achieves state-of-the-art results on six widely studied text classification datasets. \n",
    "More recently, pre-trained language models have shown to be useful in learning \n",
    "common language representations by utilizing a large amount of unlabeled data: e.g., \n",
    "OpenAI GPT (Radford et al., 2018) and BERT (Devlin et al., 2018). \n",
    "BERT is based on a multi-layer bidirectional Transformer (Vaswani et al., 2017) and is \n",
    "trained on plain text for masked word prediction and next sentence prediction tasks.\n",
    "\"\"\"\n",
    "\n",
    "t1=time.time()\n",
    "\n",
    "model = Summarizer()\n",
    "result = model(text, num_sentences=3)  \n",
    "output = ''.join(result)\n",
    "\n",
    "t2=time.time()\n",
    "\n",
    "print (\"\\n\", output)\n",
    "print('\\ntime for BERT:', t2-t1, \"sec\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from summarizer.bert import Summarizer ,TransformerSummarizer\n",
    "\n",
    "t1=time.time()\n",
    "\n",
    "GPT2_model = TransformerSummarizer(transformer_type=\"GPT2\",transformer_model_key=\"gpt2-medium\")\n",
    "output = ''.join(GPT2_model(text, num_sentences=3))\n",
    "\n",
    "t2=time.time()\n",
    "print (\"\\n\", output)\n",
    "\n",
    "print('\\ntime for GPT2:', t2-t1, \"sec\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "t1=time.time()\n",
    "\n",
    "model = TransformerSummarizer(transformer_type=\"XLNet\",transformer_model_key=\"xlnet-base-cased\")\n",
    "output = ''.join(model(text, num_sentences=3))\n",
    "\n",
    "t2=time.time()\n",
    "\n",
    "print (\"\\n\",output)\n",
    "\n",
    "print('\\ntime for XLNet:', t2-t1, \"sec\" )#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# abstractive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "import torch\n",
    "import time\n",
    "\n",
    "min_length=20\n",
    "\n",
    "\n",
    "t1=time.time()\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-large')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-large')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "preprocess_text = text.strip().replace(\"\\n\",\"\")\n",
    "t5_prepared_Text = \"summarize: \"+preprocess_text\n",
    "\n",
    "\n",
    "tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "\n",
    "summary_ids = model.generate(tokenized_text,\n",
    "                                    num_beams=4,\n",
    "                                    no_repeat_ngram_size=2,\n",
    "                                    min_length=20,\n",
    "                                    max_length= min_length*5 ,\n",
    "                                    early_stopping=True)\n",
    "\n",
    "output_t5 = tokenizer.decode(summary_ids[0], skip_special_tokens=True, max_length=2)\n",
    "               \n",
    "t2=time.time()       \n",
    "\n",
    "print (\"\\n\",output_t5)\n",
    "\n",
    "print('\\ntime for T5:', t2-t1, \"sec\" )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import time\n",
    "\n",
    "t1=time.time()\n",
    "\n",
    "min_length=20\n",
    "\n",
    "summarization = pipeline(\"summarization\", model= \"t5-large\")\n",
    "output_t= summarization(text, max_length= min_length*5,  min_length=20, do_sample=False)[0]['summary_text']\n",
    "\n",
    "\n",
    "t2=time.time()\n",
    "\n",
    "print (\"\\n\", output_t)\n",
    "\n",
    "print('\\ntime for T5:', t2-t1, \"sec\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "import torch\n",
    "import time\n",
    "\n",
    "min_length=20\n",
    "\n",
    "t1=time.time()\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "preprocess_text = text.strip().replace(\"\\n\",\"\")\n",
    "\n",
    "tokenized_text = tokenizer.encode(preprocess_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "summary_ids = model.generate(tokenized_text,\n",
    "                                    num_beams=4,\n",
    "                                    no_repeat_ngram_size=2,\n",
    "                                    min_length=20,\n",
    "                                    max_length= min_length*5,\n",
    "                                    early_stopping=True)\n",
    "\n",
    "output_br = tokenizer.decode(summary_ids[0], skip_special_tokens=True, max_length=2)\n",
    "          \n",
    "t2=time.time()       \n",
    "\n",
    "print (\"\\n\",output_br)\n",
    "\n",
    "print('\\ntime for BART:', t2-t1, \"sec\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "t1=time.time()\n",
    "\n",
    "min_length=20\n",
    "\n",
    "summarization = pipeline(\"summarization\", model= \"facebook/bart-large-cnn\")\n",
    "output_b = summarization(text, max_length= min_length*5, min_length=20, do_sample=False)[0]['summary_text']\n",
    "\n",
    "t2=time.time()\n",
    "\n",
    "print (\"\\n\",output_b)\n",
    "\n",
    "print('\\ntime for BART:', t2-t1, \"sec\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pegasus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "import torch\n",
    "\n",
    "import time\n",
    "\n",
    "t1=time.time()\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "min_length=20\n",
    "\n",
    "pegasus_model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-cnn_dailymail\").to(device)\n",
    "pegasus_tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-cnn_dailymail\")\n",
    "\n",
    "input_text = ' '.join(text.split())\n",
    "batch = pegasus_tokenizer.prepare_seq2seq_batch(input_text, truncation=True,\n",
    "                                                 padding='longest', return_tensors=\"pt\").to(device)\n",
    "\n",
    "summary_ids = pegasus_model.generate(**batch,\n",
    "                                        num_beams=4,\n",
    "                                        num_return_sequences=1,\n",
    "                                        no_repeat_ngram_size = 2,\n",
    "                                        length_penalty = 1,\n",
    "                                        min_length = 20,\n",
    "                                        max_length = min_length*5,\n",
    "                                        early_stopping = True)\n",
    "\n",
    "summ = ''.join (pegasus_tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)[0])\n",
    "output_p= summ.replace(\"<n>\", \" \")\n",
    "\n",
    "t2=time.time()\n",
    "\n",
    "print (\"\\n\", output_p)\n",
    "\n",
    "print('\\ntime for Pagasus:', t2-t1, \"sec\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "\n",
    "import time\n",
    "\n",
    "min_length=20\n",
    "\n",
    "t1=time.time()\n",
    "\n",
    "tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-cnn_dailymail\")\n",
    "model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-cnn_dailymail\") \n",
    "\n",
    "\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "\n",
    "output_Pegasus = model.generate(\n",
    "    input_ids, \n",
    "    min_length=20,\n",
    "    max_length= min_length*5,\n",
    "    num_beams=4, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "decode_text= tokenizer.decode(output_Pegasus[0], skip_special_tokens=True)\n",
    "\n",
    "summ= decode_text.replace(\"<n>\", \" \")\n",
    "\n",
    "t2=time.time()\n",
    "\n",
    "print (\"\\n\", summ)\n",
    "\n",
    "print('\\ntime for Pagasus:', t2-t1, \"sec\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LEDTokenizer, LEDForConditionalGeneration\n",
    "import time\n",
    "\n",
    "min_length=20\n",
    "\n",
    "\n",
    "\n",
    "t1=time.time()\n",
    "\n",
    "\n",
    "device = torch.device('cpu')\n",
    "    \n",
    "model = LEDForConditionalGeneration.from_pretrained(\"allenai/led-large-16384-arxiv\")\n",
    "tokenizer = LEDTokenizer.from_pretrained(\"allenai/led-large-16384-arxiv\")\n",
    "\n",
    "inputs_led= tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "global_attention_mask = torch.zeros_like(inputs_led)\n",
    "global_attention_mask[:, 0] = 1\n",
    "\n",
    "summary_ids = model.generate(inputs_led, global_attention_mask=global_attention_mask,\n",
    "                             num_beams=4,\n",
    "                             min_length=20,\n",
    "                             max_length= 100)\n",
    "\n",
    "output_led= tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "\n",
    "t2=time.time()\n",
    "\n",
    "print (\"\\n\", output_led)\n",
    "\n",
    "print('\\ntime for LED:', t2-t1, \"sec\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flask API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from summarizer import Summarizer,TransformerSummarizer\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "import torch\n",
    "from transformers import LEDTokenizer, LEDForConditionalGeneration\n",
    "import torch\n",
    "import json \n",
    "\n",
    "\n",
    "\n",
    "from flask import Flask, request, render_template\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "\n",
    "def my_form():\n",
    "    return render_template('index6.html')\n",
    "\n",
    "\n",
    "@app.route(\"/get\", methods=[\"POST\"])\n",
    "\n",
    "\n",
    "def my_form_post():\n",
    "    text = request.form['input']\n",
    "    num = int(request.form['sentNumber'])\n",
    "    Wcount = int( request.form['wordCount'])\n",
    "    absmodel = request.form['absModel']\n",
    "    exmodel = request.form['extModel']\n",
    "    type_sum= request.form['type']\n",
    "    \n",
    "    \n",
    "    if type_sum=='extractive':\n",
    "        \n",
    "        if exmodel == \"BERT\":\n",
    "            \n",
    "            sum_text= summary_BERT(text, num)\n",
    "            \n",
    "            print(\"BERT\")\n",
    "            \n",
    "        elif exmodel == \"GPT2\":\n",
    "            \n",
    "             sum_text=  summary_GPT2(text, num)\n",
    "                \n",
    "             print(\"GPT2\")   \n",
    "                \n",
    "        elif exmodel == \"XLNet\":   \n",
    "            \n",
    "            sum_text=  summary_XLNet(text, num)\n",
    "            \n",
    "            print(\"XLNet\")\n",
    "            \n",
    "\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        if absmodel == \"T5\":\n",
    "            \n",
    "            sum_text= summary_T5(text, Wcount)\n",
    "            \n",
    "            print(\"T5\")\n",
    "            \n",
    "            \n",
    "        elif  absmodel == \"BART\":\n",
    "            \n",
    "            sum_text= summary_BART(text, Wcount)\n",
    "            \n",
    "            print(\"BART\")\n",
    "            \n",
    "        elif  absmodel == \"Pegasus\": \n",
    "            \n",
    "            sum_text= summary_Pegasus(text, Wcount)\n",
    "            print(\"Pegasus\")\n",
    "            \n",
    "        elif absmodel==\"LED\":   \n",
    "    \n",
    "             sum_text= summary_LED(text, Wcount)\n",
    "        \n",
    "             print(\"LED\")\n",
    "      \n",
    "    \n",
    "    return sum_text\n",
    "\n",
    "\n",
    "def summary_BERT(body, num_sent):\n",
    "    \n",
    "    model = Summarizer()\n",
    "    result = model(body, num_sentences=num_sent)  # Number of sentences\n",
    "    full = ''.join(result) \n",
    "    return full\n",
    "\n",
    "\n",
    "def summary_GPT2(body, num_sent):\n",
    "    \n",
    "    GPT2_model = TransformerSummarizer(transformer_type=\"GPT2\",transformer_model_key=\"gpt2-medium\")\n",
    "    full = ''.join(GPT2_model(body, num_sentences= num_sent))\n",
    "     \n",
    "    return full\n",
    "\n",
    "\n",
    "def summary_XLNet(body, num_sent):\n",
    "    model = TransformerSummarizer(transformer_type=\"XLNet\",transformer_model_key=\"xlnet-base-cased\")\n",
    "    full = ''.join(model(body, num_sentences=num_sent))\n",
    "\n",
    "    return full\n",
    "\n",
    "\n",
    "\n",
    "def summary_T5(text, min_length):\n",
    "    \n",
    "    #summarization = pipeline(\"summarization\", model= \"t5-large\")\n",
    "    #full = summarization(text, max_length=len(text),  min_length=10, do_sample=False)[0]['summary_text']\n",
    "    \n",
    "    model = T5ForConditionalGeneration.from_pretrained('t5-large')\n",
    "    tokenizer = T5Tokenizer.from_pretrained('t5-large')\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "    preprocess_text = text.strip().replace(\"\\n\",\"\")\n",
    "    t5_prepared_Text = \"summarize: \"+preprocess_text\n",
    "\n",
    "    tokenized_text = tokenizer.encode(t5_prepared_Text, truncation=True,  return_tensors=\"pt\").to(device)\n",
    "\n",
    "    summary_ids = model.generate(tokenized_text,\n",
    "                                    num_beams=4,\n",
    "                                    no_repeat_ngram_size=2,\n",
    "                                    num_return_sequences=1,\n",
    "                                    min_length= min_length,\n",
    "                                    max_length= min_length*5,\n",
    "                                    early_stopping=True)\n",
    "\n",
    "    full = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "    return full\n",
    "\n",
    "\n",
    "def summary_BART(text, min_length):\n",
    "    \n",
    "    summarization = pipeline(\"summarization\", model= \"facebook/bart-large-cnn\")\n",
    "    \n",
    "    full = summarization(text, max_length= min_length*5,  min_length= min_length, do_sample=False)[0]['summary_text']\n",
    "    \n",
    "    \n",
    "    #model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "    #tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "    #device = torch.device('cpu')\n",
    "    #preprocess_text = text.strip().replace(\"\\n\",\"\")\n",
    "    #BART_prepared_Text = \"summarize: \"+preprocess_text\n",
    "    #tokenized_text = tokenizer.encode(text, truncation=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    #summary_ids = model.generate(tokenized_text,\n",
    "                                        #num_beams=4,\n",
    "                                        #no_repeat_ngram_size=2,\n",
    "                                        #num_return_sequences=1,\n",
    "                                        #min_length=min_length,\n",
    "                                        #max_length=min_length*3,\n",
    "                                        #length_penalty = 1,\n",
    "                                        #early_stopping=True)\n",
    "\n",
    "    #full = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return full\n",
    "\n",
    "\n",
    "\n",
    "def summary_Pegasus(text, min_length):\n",
    "    \n",
    "    \n",
    "     \n",
    "    device = torch.device('cpu')\n",
    "\n",
    "    pegasus_model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-cnn_dailymail\").to(device)\n",
    "    pegasus_tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-cnn_dailymail\")\n",
    "\n",
    "    #input_text = ' '.join(text.split())\n",
    "    \n",
    "    input_ids = pegasus_tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    # Generate the output (Here, we use beam search but you can also use any other strategy you like)\n",
    "    output = pegasus_model.generate(\n",
    "        input_ids, \n",
    "        num_beams=4, \n",
    "        early_stopping=True,\n",
    "        min_length = min_length,\n",
    "        max_length = min_length*5,\n",
    "        no_repeat_ngram_size = 2)\n",
    "\n",
    "  \n",
    "    summ = ''.join (pegasus_tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "    \n",
    "    #batch = pegasus_tokenizer.prepare_seq2seq_batch(input_text, truncation=True,\n",
    "                                                 #padding='longest', return_tensors=\"pt\").to(device)\n",
    "    #summary_ids = pegasus_model.generate(**batch,\n",
    "                                        #num_beams=4,\n",
    "                                        #num_return_sequences=1,\n",
    "                                        #no_repeat_ngram_size = 2,\n",
    "                                        #length_penalty = 1,\n",
    "                                        #min_length = min_length,\n",
    "                                        #max_length = len(input_text),\n",
    "                                        #early_stopping = True)\n",
    "\n",
    "    #summ = ''.join (pegasus_tokenizer.batch_decode(summary_ids, skip_special_tokens=True,\n",
    "                                                   #clean_up_tokenization_spaces=True)[0])\n",
    "    full= summ.replace(\"<n>\", \" \")\n",
    "    \n",
    "    return full\n",
    "\n",
    "\n",
    "\n",
    "def summary_LED(text,  min_length):\n",
    "    \n",
    "    device = torch.device('cpu')\n",
    "\n",
    "    model = LEDForConditionalGeneration.from_pretrained(\"allenai/led-large-16384-arxiv\")\n",
    "    tokenizer = LEDTokenizer.from_pretrained(\"allenai/led-large-16384-arxiv\")\n",
    "\n",
    "\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Global attention on the first token (cf. Beltagy et al. 2020)\n",
    "    global_attention_mask = torch.zeros_like(inputs)\n",
    "    global_attention_mask[:, 0] = 1\n",
    "\n",
    "       # Generate Summary\n",
    "    summary_ids = model.generate(inputs, global_attention_mask=global_attention_mask,\n",
    "                                 num_beams=3, min_length=min_length, max_length=min_length*5)\n",
    "    full= tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "                                 \n",
    "    return full\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
